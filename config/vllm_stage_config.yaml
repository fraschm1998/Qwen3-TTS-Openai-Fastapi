# vLLM-Omni stage config for Qwen3-TTS concurrent inference
# Increase max_batch_size and max_num_seqs for multiple concurrent users.
# Adjust gpu_memory_utilization based on your VRAM (0.8 = ~19GB on a 24GB card).
#
# See: https://docs.vllm.ai/projects/vllm-omni/en/latest/configuration/stage_configs/

stage_args:
  - stage_id: 0
    stage_type: llm
    runtime:
      process: true
      devices: "0"
      max_batch_size: 12
    engine_args:
      model_stage: qwen3_tts
      model_arch: Qwen3TTSForConditionalGeneration
      worker_type: generation
      scheduler_cls: vllm_omni.core.sched.omni_generation_scheduler.OmniGenerationScheduler
      enforce_eager: true
      trust_remote_code: true
      async_scheduling: false
      enable_prefix_caching: false
      engine_output_type: audio
      gpu_memory_utilization: 0.8
      distributed_executor_backend: mp
      max_num_batched_tokens: 1000000
      max_num_seqs: 12
      async_chunk: false
    final_output: true
    final_output_type: audio
