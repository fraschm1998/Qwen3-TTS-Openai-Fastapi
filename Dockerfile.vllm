# vLLM-Omni Dockerfile for Qwen3-TTS
# Uses official vLLM-Omni image (Python 3.12, CUDA, continuous batching)
# See: https://docs.vllm.ai/projects/vllm-omni/en/latest/user_guide/examples/offline_inference/qwen3_tts/

FROM vllm/vllm-omni:v0.14.0rc1

WORKDIR /app

# Install system deps for audio processing
RUN rm -rf /var/lib/apt/lists/* && \
    apt-get clean && \
    apt-get update --allow-releaseinfo-change -o Acquire::AllowInsecureRepositories=true -o Acquire::AllowDowngradeToInsecureRepositories=true 2>/dev/null || true && \
    apt-get install -y --allow-unauthenticated --no-install-recommends \
    ffmpeg \
    libsndfile1 \
    libsox-dev \
    sox \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy dependency files first for layer caching
COPY pyproject.toml README.md ./

# Install all deps from pyproject.toml (torch/vllm already in base image)
RUN pip install --no-cache-dir -U pip && \
    pip install --no-cache-dir ".[api]"

# Copy application code
COPY . .

# Register package
RUN pip install --no-cache-dir --no-deps -e .

ENV HOST=0.0.0.0
ENV PORT=8880
ENV WORKERS=1
ENV TTS_BACKEND=vllm_omni
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn
ENV PYTHONUNBUFFERED=1

EXPOSE 8880

HEALTHCHECK --interval=30s --timeout=10s --start-period=180s --retries=3 \
    CMD curl -f http://localhost:8880/health || exit 1

CMD ["python", "-m", "api.main"]
